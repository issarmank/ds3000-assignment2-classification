{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489182b0-8e57-45cf-a353-8c6e605be145",
   "metadata": {
    "id": "489182b0-8e57-45cf-a353-8c6e605be145"
   },
   "source": [
    "# Grade: 100 points\n",
    "\n",
    "# Assignment 02: Classification with Ridge, LASSO, and Feedforward Neural Networks\n",
    "\n",
    "## Instructions\n",
    "\n",
    "#### Follow these steps before submitting your assignment :\n",
    "\n",
    "1. Complete the notebook.\n",
    "\n",
    "2. Make sure all plots have axis labels.\n",
    "\n",
    "3. Once the notebook is complete, `Restart` your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "4. Fix any errors until your notebook runs without any problems.\n",
    "\n",
    "5. Please note, a random seed of 42 needs to be set to ensure the reproducability of the results -- *DO NOT* change this random seed. **If you call additional functions that are based on random number generators, you will need to define their seed to 42 as well**.\n",
    "\n",
    "6. Make sure to reference all external code and documentation used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f6842d-cad5-449e-9e2e-ff92da30ba73",
   "metadata": {
    "executionInfo": {
     "elapsed": 12884,
     "status": "ok",
     "timestamp": 1760023910878,
     "user": {
      "displayName": "Natalie Filep",
      "userId": "05816467162930162568"
     },
     "user_tz": 240
    },
    "id": "04f6842d-cad5-449e-9e2e-ff92da30ba73"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "accuracy_score, precision_score, recall_score, f1_score,\n",
    "roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Suppress overflow warnings from sklearn/numpy\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "np.seterr(over='ignore')\n",
    "\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "#Check device for NN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf3d6e-c363-43f4-b4e9-b3034cc237ec",
   "metadata": {
    "id": "90bf3d6e-c363-43f4-b4e9-b3034cc237ec"
   },
   "source": [
    "# Q1 - Data Loading and Exploration\n",
    "\n",
    "[This Kaggle dataset](https://www.kaggle.com/competitions/titanic) contains data on passengers of the famous Titanic disaster. The task is to examine and predict what sorts of people were more likely to survive using data on their name, age, gender, socio-economic class, etc.\n",
    "\n",
    "#### 1. Loading our Data\n",
    "Load the train and test data into a pandas dataframe and display the first few rows using ``.head()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1059936e-0131-432d-8270-f72a57e6e046",
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1760024160406,
     "user": {
      "displayName": "Natalie Filep",
      "userId": "05816467162930162568"
     },
     "user_tz": 240
    },
    "id": "1059936e-0131-432d-8270-f72a57e6e046"
   },
   "outputs": [],
   "source": [
    "#Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a011de-a0ac-4157-89ec-28a5ab845de5",
   "metadata": {
    "id": "26a011de-a0ac-4157-89ec-28a5ab845de5"
   },
   "source": [
    "#### 2. Checking for Null Values\n",
    "Check for null values in the training data using ``.isnull().sum()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53eb9c4-437e-411e-9328-5db222f2825e",
   "metadata": {
    "id": "a53eb9c4-437e-411e-9328-5db222f2825e"
   },
   "outputs": [],
   "source": [
    "#Q1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf5cba-f6f0-4a2e-b0ad-9f42df345e5d",
   "metadata": {
    "id": "0adf5cba-f6f0-4a2e-b0ad-9f42df345e5d"
   },
   "source": [
    "#### 3. Visualizing Null Values\n",
    "\n",
    "Create a seaborn heatmap that displays all of the null data using ``sns.heatmap(train.isnull(), cbar=False)`` to get a visual representation on how much data is missing for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532fdcb-53c8-4dd7-ac20-47aa9e1b87fa",
   "metadata": {
    "id": "9532fdcb-53c8-4dd7-ac20-47aa9e1b87fa"
   },
   "outputs": [],
   "source": [
    "#Q1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990feddd-6767-487d-9217-0854cca722c2",
   "metadata": {
    "id": "990feddd-6767-487d-9217-0854cca722c2"
   },
   "source": [
    "#### 4a. Visualizing Survival vs. Sex\n",
    "\n",
    "Create a seaborn barplot that compares 'Survival Rate by Sex', where x is 'Sex' and y is 'Survived' from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d9cf6-c16c-409a-a243-300edcb5287e",
   "metadata": {
    "id": "6b7d9cf6-c16c-409a-a243-300edcb5287e"
   },
   "outputs": [],
   "source": [
    "#Q1.4a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ab1bc-96d5-4df4-a9ce-4e766e8f6bf8",
   "metadata": {
    "id": "759ab1bc-96d5-4df4-a9ce-4e766e8f6bf8"
   },
   "source": [
    "#### 4b. Visualizing Survival vs. Passenger Class\n",
    "Create another barplot for 'Survival Rate by Passenger Class' (the `Pclass` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3eb47e-b44b-4180-a826-e44639ae7852",
   "metadata": {
    "id": "af3eb47e-b44b-4180-a826-e44639ae7852"
   },
   "outputs": [],
   "source": [
    "#Q1.4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd1f30-1dc8-4e18-9f25-9b315381d4f3",
   "metadata": {
    "id": "10dd1f30-1dc8-4e18-9f25-9b315381d4f3"
   },
   "source": [
    "#### 5. Visualizing Age Distribution by Survival\n",
    "\n",
    "Create a seaborn a stacked histogram for 'Age Distribution by Survival'. x is 'Age', and one histogram should be for 'Survived'=0, and the other for 'Survived'=1. Use a bin size of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d222969-9342-4f71-8388-5f377a9587fd",
   "metadata": {
    "id": "4d222969-9342-4f71-8388-5f377a9587fd"
   },
   "outputs": [],
   "source": [
    "#Q1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c764b6d-92b4-463d-af6a-018be47d64ee",
   "metadata": {
    "id": "0c764b6d-92b4-463d-af6a-018be47d64ee"
   },
   "source": [
    "#### 6. Feature Correlation\n",
    "\n",
    "Create a seaborn heatmap: 'Feature Correlation Heatmap' which displays only numeric variables. The result should contain 7 variables. Although we will not be doing so in this assignment, noting which variables are most highly correlated can inform our decisions on feature engineering. We will discuss this further in assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4385446-c42d-495d-bd44-4050ce7eb7f2",
   "metadata": {
    "id": "e4385446-c42d-495d-bd44-4050ce7eb7f2"
   },
   "outputs": [],
   "source": [
    "#Q1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f271d52-ed80-42e7-b4f1-928ae67bcd7e",
   "metadata": {
    "id": "7f271d52-ed80-42e7-b4f1-928ae67bcd7e"
   },
   "source": [
    "# Q2 - Feature Engineering\n",
    "\n",
    "In this section, you will engineer new features from the Titanic dataset to help your model capture more meaningful relationships.\n",
    "\n",
    "Use **pandas** functions such as:\n",
    "- `.apply()`\n",
    "- `.replace()`\n",
    "- `.groupby()`\n",
    "- `.median()`\n",
    "- `.fillna()`\n",
    "- `.map()`\n",
    "- `.get_dummies()`\n",
    "- `.astype()`\n",
    "\n",
    "Follow the outline below and implement each transformation step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14272eca-00be-4870-be0f-7bb14472d709",
   "metadata": {
    "id": "14272eca-00be-4870-be0f-7bb14472d709"
   },
   "source": [
    "#### 1. Extract Titles from Passenger Names\n",
    "Each passenger’s `Name` contains a title (e.g., *Mr.*, *Mrs.*, *Miss.*, *Dr.*, etc.).  \n",
    "- Use `.apply()` with a helper function to extract each passenger’s title from the `Name` column.  \n",
    "- Combine less common titles ('Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona') into a single category called `\"Rare\"`.  \n",
    "  - You can do this efficiently with `.replace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389bda1-855f-4a6f-8a06-5b8fdf9a524f",
   "metadata": {
    "id": "8389bda1-855f-4a6f-8a06-5b8fdf9a524f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract Title from Name\n",
    "def extract_title(name):\n",
    "    m = re.search(r\", (.*?)\\\\.\", name)\n",
    "    return m.group(1) if m else 'Unknown'\n",
    "\n",
    "#Q2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c80ec-0fa4-45fd-b316-1fcd0be0a71e",
   "metadata": {
    "id": "cf7c80ec-0fa4-45fd-b316-1fcd0be0a71e"
   },
   "source": [
    "#### 2. Create Family-Based Features\n",
    "We can capture social context by adding two new features:\n",
    "- **`FamilySize`**: total number of family members aboard (siblings/spouses + parents/children + 1 for the passenger).  \n",
    "- **`IsAlone`**: binary indicator (`1` if alone, `0` otherwise).  \n",
    "\n",
    "To do this:\n",
    "- Use arithmetic operations on existing columns (`SibSp`, `Parch`) to create `FamilySize`.  \n",
    "- Create `IsAlone` using a boolean condition and then convert it to an integer with `.astype(int)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa12333-0fc9-4c84-bcf8-425ffd25e80e",
   "metadata": {
    "id": "aaa12333-0fc9-4c84-bcf8-425ffd25e80e"
   },
   "outputs": [],
   "source": [
    "#Q2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afdb84-696b-416c-ac58-bd57efb528f5",
   "metadata": {
    "id": "74afdb84-696b-416c-ac58-bd57efb528f5"
   },
   "source": [
    "#### 3. Impute Missing Ages\n",
    "Instead of using a single median for all passengers, compute a **median Age per Title**.  \n",
    "Then fill in missing Age values using that median for the passenger’s title group.\n",
    "\n",
    "You’ll need to:\n",
    "- Use `.groupby()` on the `Title` column and compute `.median()` for `Age`.  \n",
    "- Use `.apply()` (with a `lambda` function) to fill in missing `Age` values row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107053e-44df-4931-be1e-e074a419cba2",
   "metadata": {
    "id": "1107053e-44df-4931-be1e-e074a419cba2"
   },
   "outputs": [],
   "source": [
    "#Q2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4271bb-b26e-4e57-b7b1-8738efff5941",
   "metadata": {
    "id": "ae4271bb-b26e-4e57-b7b1-8738efff5941"
   },
   "source": [
    "#### 4. Fill Missing Fares\n",
    "Fill missing `Fare` values with the overall median fare of the dataset using `.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4562d-a99f-4cc8-99b7-8cf023fc287d",
   "metadata": {
    "id": "55f4562d-a99f-4cc8-99b7-8cf023fc287d"
   },
   "outputs": [],
   "source": [
    "#Q2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ae99b-edd9-4deb-839e-90b5dbcff6d4",
   "metadata": {
    "id": "d96ae99b-edd9-4deb-839e-90b5dbcff6d4"
   },
   "source": [
    "#### 5. Encode Categorical Variables\n",
    "Machine learning models need numeric features. Convert categorical features into numeric form:\n",
    "- Convert `Sex` into 0/1 values using `.map()`.  \n",
    "- One-hot encode `Embarked` into dummy variables (e.g., `Embarked_C`, `Embarked_Q`, `Embarked_S`) using `pd.get_dummies()` with the `columns` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55cfec-10bf-4e03-9e8d-9d5a7463fb59",
   "metadata": {
    "id": "ce55cfec-10bf-4e03-9e8d-9d5a7463fb59"
   },
   "outputs": [],
   "source": [
    "#Q2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c47bb-cc1c-46da-9421-48d950780344",
   "metadata": {
    "id": "2f7c47bb-cc1c-46da-9421-48d950780344"
   },
   "source": [
    "#### 6. Define Your Feature Set\n",
    "Create your final feature matrix `X` and label vector `y`.  \n",
    "Select a combination of numerical and engineered features (e.g., `Pclass`, `Sex`, `Age`, `Fare`, `FamilySize`, `IsAlone`, `Embarked`(_C, _Q, _S) columns).\n",
    "\n",
    "Use `X.head()` to verify your final dataset looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8c729-5909-431a-adcd-16ca4010d8d9",
   "metadata": {
    "id": "06e8c729-5909-431a-adcd-16ca4010d8d9"
   },
   "outputs": [],
   "source": [
    "#Q2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a923e0d-3d54-427e-9816-5f1da2ece93b",
   "metadata": {
    "id": "1a923e0d-3d54-427e-9816-5f1da2ece93b"
   },
   "source": [
    "# Q3 - Train/Test Split and Feature Scaling\n",
    "\n",
    "Now that you’ve finished feature engineering, it’s time to prepare the data for modeling.  \n",
    "This step ensures that your model generalizes well and that all numeric features are on a similar scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f85b1b-822b-4719-b6e4-cb8a8e78d97b",
   "metadata": {
    "id": "73f85b1b-822b-4719-b6e4-cb8a8e78d97b"
   },
   "source": [
    "#### 1. Split the Data\n",
    "Use the **`train_test_split()`** function from `sklearn.model_selection` to divide your data into:\n",
    "- **Training set (80%)**\n",
    "- **Testing set (20%)**\n",
    "\n",
    "Make sure to:\n",
    "- Use the `stratify` parameter to preserve the class balance of the target variable (`y`).\n",
    "- Set a fixed `random_state` so results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3986c-9277-4316-a83c-a0d5bbea6f7e",
   "metadata": {
    "id": "18b3986c-9277-4316-a83c-a0d5bbea6f7e"
   },
   "outputs": [],
   "source": [
    "#Q3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b312e-4e16-41fa-af9e-85be1f564d18",
   "metadata": {
    "id": "dc2b312e-4e16-41fa-af9e-85be1f564d18"
   },
   "source": [
    "#### 2. Scale the Numeric Features\n",
    "Feature scaling helps models converge faster and prevents some features from dominating others.\n",
    "\n",
    "- Use `StandardScaler` from `sklearn.preprocessing` to **standardize** numeric columns such as `Age`, `Fare`, and `FamilySize`.\n",
    "- Remember to **fit the scaler only on the training data** and then **transform both** the training and test sets.\n",
    "  - Functions to use: `.fit()` and `.transform()`\n",
    "- Overwrite the scaled columns in both `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7027872-7f76-4f43-82fe-f036776c1c46",
   "metadata": {
    "id": "a7027872-7f76-4f43-82fe-f036776c1c46"
   },
   "outputs": [],
   "source": [
    "#Q3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0987c19-b3e5-4898-bd11-453ea893849b",
   "metadata": {
    "id": "f0987c19-b3e5-4898-bd11-453ea893849b"
   },
   "source": [
    "#### 3. Prepare Data for PyTorch\n",
    "PyTorch expects input data as floating-point numbers (`float32`).  \n",
    "Also, boolean columns (`True`/`False`) need to be converted to numeric values (1.0 and 0.0).\n",
    "\n",
    "Use **`.astype(np.float32)`** on both `X_train` and `X_test` to ensure all features are numeric and compatible with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb65d17-512e-4c0a-ad61-a87b388a069c",
   "metadata": {
    "id": "9fb65d17-512e-4c0a-ad61-a87b388a069c"
   },
   "outputs": [],
   "source": [
    "#Q3.3 Cast bools to 1.0 for true, and 0.0 for false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92473a8-51c9-41b6-9bb9-6a901d2cfdaa",
   "metadata": {
    "id": "d92473a8-51c9-41b6-9bb9-6a901d2cfdaa"
   },
   "source": [
    "# Q4 - Training LASSO and Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df85193-d274-4a89-a090-d83b893ca607",
   "metadata": {
    "id": "1df85193-d274-4a89-a090-d83b893ca607"
   },
   "source": [
    "#### 1. Define the Models\n",
    "Create two logistic regression models:\n",
    "- **LASSO (L1 Regularization):**\n",
    "  - Use the parameter `penalty='l1'`\n",
    "  - Requires the `solver='liblinear'`\n",
    "- **Ridge (L2 Regularization):**\n",
    "  - Use the parameter `penalty='l2'`\n",
    "  - A good solver choice is `'lbfgs'`\n",
    "- Set `max_iter` high enough (e.g., 1000) to ensure convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb079d28-3f88-43ec-b1b1-7b8eb1bbac6f",
   "metadata": {
    "id": "bb079d28-3f88-43ec-b1b1-7b8eb1bbac6f"
   },
   "outputs": [],
   "source": [
    "#Q4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96efbb7-7468-49a7-8173-b12abfcea5c0",
   "metadata": {
    "id": "b96efbb7-7468-49a7-8173-b12abfcea5c0"
   },
   "source": [
    "#### 2. Train (Fit) Each Model\n",
    "Use the `.fit()` method to train both models on your **training data** (`X_train`, `y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b6a87-47cf-4362-b1fe-d4742aa47646",
   "metadata": {
    "id": "0a2b6a87-47cf-4362-b1fe-d4742aa47646"
   },
   "outputs": [],
   "source": [
    "#Q4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c39fa-72e9-40d9-886e-a07ab83500b2",
   "metadata": {
    "id": "3e6c39fa-72e9-40d9-886e-a07ab83500b2"
   },
   "source": [
    "#### 3. Make Predictions\n",
    "Use `.predict_proba()` on the **test set** to get predicted probabilities for the positive class (`Survived = 1`).\n",
    "\n",
    "You’ll then:\n",
    "- Extract the second column (the probability of class 1)\n",
    "- Convert these probabilities to binary predictions (`0` or `1`) using a threshold of 0.5  \n",
    "  - You can do this using array comparison and `.astype(int)`\n",
    "\n",
    "Predict 1 if probability > 0.5 else predict 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da55c9b-684a-444c-960a-ee82cf38a8f8",
   "metadata": {
    "id": "1da55c9b-684a-444c-960a-ee82cf38a8f8"
   },
   "outputs": [],
   "source": [
    "#Q4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f18df1-60be-4e9d-9d89-1979665389a5",
   "metadata": {
    "id": "40f18df1-60be-4e9d-9d89-1979665389a5"
   },
   "source": [
    "Run the following for evaluation metrics for both of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcf1f3-a440-4994-a9d0-bba83f2b47e9",
   "metadata": {
    "id": "c0bcf1f3-a440-4994-a9d0-bba83f2b47e9"
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def evaluate_model(name, y_true, y_pred, y_prob):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"{name} → Accuracy: {acc:.4f}, F1: {f1:.4f}, ROC AUC: {auc:.4f}\")\n",
    "\n",
    "evaluate_model('LASSO Logistic Regression', y_test, lasso_preds, lasso_probs)\n",
    "evaluate_model('Ridge Logistic Regression', y_test, ridge_preds, ridge_probs)\n",
    "\n",
    "# Coefficient comparison\n",
    "coeff_df = pd.DataFrame({\n",
    "'Feature': features,\n",
    "'LASSO Coeff': lasso.coef_[0],\n",
    "'Ridge Coeff': ridge.coef_[0]\n",
    "})\n",
    "\n",
    "coeff_df_melted = coeff_df.melt(id_vars='Feature', var_name='Model', value_name='Coefficient')\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=coeff_df_melted, x='Feature', y='Coefficient', hue='Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Feature Coefficients: LASSO vs Ridge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fbb755-f999-4e65-9fbd-0c2e6c891df8",
   "metadata": {
    "id": "42fbb755-f999-4e65-9fbd-0c2e6c891df8"
   },
   "source": [
    "# Q5 - Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc808d-a334-4a9f-8fb5-2d9c2d51ee3d",
   "metadata": {
    "id": "19dc808d-a334-4a9f-8fb5-2d9c2d51ee3d"
   },
   "source": [
    "Convert the datasets to tensors for Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb4f2e-82b3-4a40-80ef-4f362e3ca773",
   "metadata": {
    "id": "08bb4f2e-82b3-4a40-80ef-4f362e3ca773"
   },
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1c6f0-bbd0-4945-b5d6-7678459d5d88",
   "metadata": {
    "id": "bff1c6f0-bbd0-4945-b5d6-7678459d5d88"
   },
   "source": [
    "Define our NN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdf5cd-f75a-4a75-bb11-270586076950",
   "metadata": {
    "id": "a8fdf5cd-f75a-4a75-bb11-270586076950"
   },
   "outputs": [],
   "source": [
    "class TitanicNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Linear(input_dim, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = TitanicNet(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457e94d-20e2-4cc6-9e1d-62a1c955239b",
   "metadata": {
    "id": "3457e94d-20e2-4cc6-9e1d-62a1c955239b"
   },
   "source": [
    "#### 1. Define your Training Loop\n",
    "\n",
    "Inside your training loop, for each batch of data from the DataLoader, you’ll need to perform the core operations that allow your neural network to learn.  \n",
    "\n",
    "At a high level, each batch should follow this sequence:\n",
    "\n",
    "1. **Reset accumulated gradients** — before processing a new batch, clear any gradients stored from the previous iteration using the appropriate optimizer method.  \n",
    "2. **Forward pass** — pass the batch of input data through the model to obtain predicted outputs (often called *logits*).  \n",
    "3. **Compute the loss** — use your chosen loss function (*criterion* in the cell above) to compare the predictions with the actual target labels.  \n",
    "4. **Backpropagation** — compute gradients of the loss with respect to the model’s parameters.  \n",
    "5. **Parameter update** — use the optimizer to apply those gradients and update the model weights.\n",
    "\n",
    "Each of these steps corresponds to a single PyTorch function call, and they must occur in this exact order for the model to train correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b0420-2cff-4652-9e4e-9779bcd4ee68",
   "metadata": {
    "id": "416b0420-2cff-4652-9e4e-9779bcd4ee68"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_losses, train_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total = 0, 0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        #Q5.1 Add training code here\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = total_correct / total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392af5b6-f9a7-4a1d-bff0-21827fb29ce3",
   "metadata": {
    "id": "392af5b6-f9a7-4a1d-bff0-21827fb29ce3"
   },
   "source": [
    "Plot training loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35744f1-4d73-480b-95c4-5a7c133e539a",
   "metadata": {
    "id": "e35744f1-4d73-480b-95c4-5a7c133e539a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f20a0-524c-4bf8-b25e-53d14f76d824",
   "metadata": {
    "id": "543f20a0-524c-4bf8-b25e-53d14f76d824"
   },
   "source": [
    "Plot training loss and accuracy again, but smoothed so we can see overall trends more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d0e6f-b6ac-49ef-bd64-f36645351056",
   "metadata": {
    "id": "ac3d0e6f-b6ac-49ef-bd64-f36645351056"
   },
   "outputs": [],
   "source": [
    "window = 100  # moving average window\n",
    "smoothed_acc = pd.Series(train_accs).rolling(window).mean()\n",
    "smoothed_loss = pd.Series(train_losses).rolling(window).mean()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(smoothed_loss, label=f'Train Loss (avg {window})')\n",
    "plt.title('Training Loss (Smoothed)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(smoothed_acc, label=f'Train Accuracy (avg {window})')\n",
    "plt.title('Training Accuracy (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8351130-9312-4c77-bfb5-ee23f1aa4cea",
   "metadata": {
    "id": "e8351130-9312-4c77-bfb5-ee23f1aa4cea"
   },
   "source": [
    "Report evaluation metrics just like we did for LASSO and Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8adaf-6c34-4a73-b60d-e24048a471dc",
   "metadata": {
    "id": "b6f8adaf-6c34-4a73-b60d-e24048a471dc"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t.to(device)).cpu().numpy().flatten()\n",
    "\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "preds = (probs > 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "prec = precision_score(y_test, preds)\n",
    "rec = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "roc_auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0370b4-36da-4b31-95c9-ea3cef395721",
   "metadata": {
    "id": "dc0370b4-36da-4b31-95c9-ea3cef395721"
   },
   "source": [
    "#### 2. ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "The **ROC curve** shows the trade-off between the **True Positive Rate (TPR)** and the **False Positive Rate (FPR)** at various classification thresholds.\n",
    "\n",
    "- Use `roc_curve()` from `sklearn.metrics` to calculate the `fpr` and `tpr` values.\n",
    "- Plot these values using `matplotlib.pyplot` to visualize your model’s ability to distinguish between classes.\n",
    "- Add an **AUC (Area Under the Curve)** score to your plot legend to summarize overall performance.\n",
    "\n",
    "A higher AUC (closer to 1.0) means your model is better at distinguishing survivors from non-survivors.\n",
    "\n",
    "Include:\n",
    "- `plt.plot()` for the ROC line  \n",
    "- Axis labels (`plt.xlabel`, `plt.ylabel`)  \n",
    "- A title and legend for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce4129-e00f-498d-bdb8-6e15f383e110",
   "metadata": {
    "id": "10ce4129-e00f-498d-bdb8-6e15f383e110"
   },
   "outputs": [],
   "source": [
    "#Q5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc269898-313e-4e81-ad5f-254e54e9a76f",
   "metadata": {
    "id": "fc269898-313e-4e81-ad5f-254e54e9a76f"
   },
   "source": [
    "#### 3. Confusion Matrix\n",
    "\n",
    "A **confusion matrix** shows how many predictions were correct versus incorrect for each class.\n",
    "\n",
    "- Use `confusion_matrix()` from `sklearn.metrics` to generate the matrix.\n",
    "- Then use `seaborn.heatmap()` to visualize it in an easy-to-read grid format.\n",
    "- Label the axes to clearly indicate which side represents predicted vs. actual values.\n",
    "\n",
    "Set `annot=True` to display the numbers inside the boxes and use a color map (e.g., `'Blues'`) for better contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3578e8b-f69e-4b27-a6c7-f56e893df6ae",
   "metadata": {
    "id": "b3578e8b-f69e-4b27-a6c7-f56e893df6ae"
   },
   "outputs": [],
   "source": [
    "#Q5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24925332-56a4-40b2-a3ae-bdd621a5af93",
   "metadata": {
    "id": "24925332-56a4-40b2-a3ae-bdd621a5af93"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10 (DS3000env)",
   "language": "python",
   "name": "ds3000env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
